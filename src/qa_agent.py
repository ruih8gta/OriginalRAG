from langgraph.graph import START, END, StateGraph
from typing_extensions import TypedDict
import model_settings
from langchain_core.prompts import ChatPromptTemplate
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool
from typing import Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages.tool import ToolMessage
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import os
import sys
import configparser  # config.iniãƒ•ã‚¡ã‚¤ãƒ«
import json
from typing import Literal
from loggings import setup_logger, log_message, handle_exception

from create_vectordb import set_rag_data_with_vector
from create_keyworddb import set_rag_data_with_keyword
import model_settings
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

# Flaskç¨¼åƒæ™‚ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
#config_path ="./config/config.ini"
# ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œæ™‚ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
config_path ="../config/config.ini"

config = configparser.ConfigParser()
config.read(config_path, encoding='utf-8') 

current_dir = os.path.dirname(__file__)
parent_dir = os.path.dirname(current_dir)

log_path = os.path.join(parent_dir,config['log']['LOG_PATH'])

doc_db_path = os.path.join(parent_dir,config['doc']['DB_DIR'],"all")

# ãƒ­ã‚°ã®è¨­å®š
logger = setup_logger(log_path)
sys.excepthook = handle_exception
log_message(logger, 'ãƒ„ãƒ¼ãƒ«èµ·å‹•', to_stdout=True)


#å›ç­”ä½œæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
ANSWER_SYSINT=(
"æ¬¡ã®æ–‡è„ˆï¼ˆcontextï¼‰ã®ã¿ã«åŸºã¥ã„ã¦è³ªå•ï¼ˆquestionï¼‰ã«ç­”ãˆã¦ãã ã•ã„ã€‚ç­”ãˆã¯ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:"
"æ–‡è„ˆï¼š{context}"
"è³ªå•: {question}"
)

QUESTION_SYSINT=(
"# å‘½ä»¤"
"ã‚ãªãŸã¯è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚"
"RAGã‚’ç”¨ã„ã¦è³ªå•ã«å›ç­”ã™ã‚‹å‰ã«å•ã„åˆã‚ã›æ–‡ã®å†…å®¹ã‚’åˆ†æã—ã¾ã™"
"ã“ã®å¾Œå•ã„åˆã‚ã›ã«é–¢ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚·ã‚¹ãƒ†ãƒ ã®ã‚„ã‚Šå–ã‚Š(conversation)ãŒä¸ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ä¼šè©±ã®çµŒç·¯ã‹ã‚‰ã€è³ªå•ãŒæ˜ç¢ºã«ãªã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚"
"è³ªå•ã«ãªã£ã¦ã„ãªã‹ã£ãŸã‚Šã€å•ã„åˆã‚ã›å†…å®¹ãŒä¸æ˜ç¢ºã§ã‚ã‚Œã°æ˜ç¢ºã«ã—ã¦ã»ã—ã„ç‚¹ã‚’ä¼ãˆãŸä¸Šã§èãè¿”ã™æ–‡ã‚’(response)ã«å‡ºåŠ›ã—ã¦ãã ã•ã„"
"èãè¿”ã—ãŒå¿…è¦ãªå ´åˆã¯ã€Step:ãƒãƒ£ãƒƒãƒˆã¨å‡ºåŠ›ã—ã€èãè¿”ã—ãŒä¸è¦ãªå ´åˆã¯ã€Step:æ¤œç´¢ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„"
"èãè¿”ã—æ–‡ã¯ã€è³ªå•ã®å†…å®¹ã‚’æ˜ç¢ºã«ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã‚ã‚Šã€è³ªå•ã®å†…å®¹ã‚’å¤‰ãˆãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚"
"èãè¿”ã—ãŒä¸è¦ãªå ´åˆã¯èãè¿”ã—æ–‡ã«ã¯ä½•ã‚‚å‡ºåŠ›ã›ãšã€æœ€çµ‚çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•æ–‡ã‚’ã¾ã¨ã‚ã¦(question)ã«å‡ºåŠ›ã—ã¦ãã ã•ã„"
"çµæœã¯jsonå½¢å¼ã§å‡ºåŠ›ã—ã¦ä¸‹ã•ã„ã€‚jsonã¨ã„ã†æ–‡å­—ã‚„```ã¯å«ã‚ãªã„ã§ãã ã•ã„"
"# å•ã„åˆã‚ã›æ–‡: {conversation}"
"# å‡ºåŠ›å½¢å¼"
"{{"
"  \"Step\": \"ãƒãƒ£ãƒƒãƒˆ or æ¤œç´¢\","
"  \"response\": \"å¿…è¦ã«å¿œã˜ãŸèãè¿”ã—ã®å†…å®¹\","
"  \"question\": \"è³ªå•æ–‡\""
"}}"
)


RETRIVER_SYSINT=(
"# å‘½ä»¤"
"ã‚ãªãŸã¯è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚"
"è³ªå•ã®å†…å®¹ã«å¿œã˜ã¦æ¤œç´¢ã«åˆ©ç”¨ã™ã‚‹DBã‚’é¸æŠã—ã¾ã™ã€‚"
"è³ªå•ã«å›ºæœ‰åè©ã‚„æ˜ç¢ºãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€tool_retrive_from_keyworddbã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰DBã‚’æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"
"è³ªå•ãŒæ›–æ˜§ã¾ãŸã¯è‡ªç„¶è¨€èªçš„ã§ã‚ã£ãŸã‚Šã€é¡ä¼¼è³ªå•ã‚„æ„å‘³çš„ãªãƒãƒƒãƒãƒ³ã‚°ãŒå¿…è¦ãªå ´åˆã¯ã€tool_retrive_from_vectordbã‚’ä½¿ç”¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«DBã‚’æ¤œç´¢ã—ã¦ãã ã•ã„"
#"è³ªå•ãŒã€Œèª°ãŒã€ã€Œä½•ã¨é–¢ä¿‚ã—ã¦ã„ã‚‹ã‹ã€ãªã©é–¢ä¿‚æ€§ã‚’å•ã†ã‚‚ã®ã‚„ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã«åŸºã¥ãè³ªæ¨è«–ãŒå¿…è¦ã§ã‚ã‚Œã°ã€tool_retrive_from_graphdbã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒ©ãƒ•DBã‚’æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"
)
# é–¢æ•°
# State
class State(TypedDict):
    """State representing the customer's order conversation."""
    # The step for creating career advice hearing -> check -> plan -> report 
    question:str
    context: str
    answer: str
    #step: str
    messages: Annotated[list, add_messages]
    # Flag indicating that the order is placed and completed.
    step:str # "ãƒãƒ£ãƒƒãƒˆ" "è³ªå•"ã€€"æ¤œç´¢"ã€€"å›ç­”"ã€€"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ" "çµ‚äº†"

# ãƒãƒ¼ãƒ‰
def Node_chatbot(state: State) -> State:
    """Node for the chatbot to answer questions."""
    #messages_str = "\n".join([str(msg) for msg in state["messages"]])
    #print(f"State!!{state}")

    if state["messages"]:
        #print(state["messages"])
        step = "è³ªå•"
        result = state["messages"][-1]
    else:
        result ="ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\nçµ‚ã‚ã‚ŠãŸã‘ã‚Œã°qã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\næ–°ãŸãªè³ªå•ã‚’ã™ã‚‹å ´åˆã¯uã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚" 
        step="ãƒãƒ£ãƒƒãƒˆ"   
    
    #log_message(logger, {str(state)}, to_stdout=False)
    #ãƒ­ã‚¬ãƒ¼ã«ã€step question answer ã‚’å‡ºåŠ›
    log_message(logger, f"Step: {step} lastMessage: {result}", to_stdout=False)
 
 
    return {"messages": [result],"step": step} 

def Node_question(state: State) -> State:
    """Node for the chatbot to answer questions."""
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥æ‰‹
    last_msg = state.get("messages", [])[-1]


    prompt = PromptTemplate(
        variables=["conversation"],
        template=QUESTION_SYSINT,
    )
    chat_instance = prompt | chat_instance_question | StrOutputParser()

    log_message(logger, f"Step: {state['step']} message: {state['messages']}", to_stdout=False)
    #answer = chat_instance.invoke({"question": last_msg.content})
    answer = chat_instance.invoke({"conversation": state["messages"]}) 
 
    #çµæœã‚’jsonå½¢å¼ã§å–å¾—
    q_=""
    answer_json = json.loads(answer)
    #kaprint(answer_json)

    if(answer_json["Step"] == "ãƒãƒ£ãƒƒãƒˆ"):
        step= "ãƒãƒ£ãƒƒãƒˆ"

    elif(answer_json["Step"] == "æ¤œç´¢"):
        step = "æ¤œç´¢"
        q_= answer_json["question"]
    else:#ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯åŸå‰‡ãªã—
        step = "è³ªå•"
        
    #log_message(logger, {str(state)}, to_stdout=False)
    log_message(logger, f"Step: {step} lastMessage: {answer_json['response']}", to_stdout=False)

    return {"question": q_,"step": step,"messages": [answer_json["response"]]}

def Node_retriver(state: State) -> State:
    inf_param_instance = model_settings.inf_param()  # model_settings.pyãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    
    topk = inf_param_instance.Top_k

    question = state.get("question")

    print(f"è³ªå•:{question}")
    # è³ªå•ã‚’RAGã§æ¤œç´¢ã—ã€çµæœã‚’å–å¾—ã™ã‚‹
    tool_msg  = chat_instance_retriver_with_tools.invoke(RETRIVER_SYSINT + question)

    if not hasattr(tool_msg, "tool_calls") or len(tool_msg.tool_calls) == 0:
        raise NotImplementedError(f'Do not call any tool in this Question:{question}')
    for tool_call in tool_msg.tool_calls:
        print(f"ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ï¼š{tool_call['name']}")
        log_message(logger, f"Step: {state['step']} lastToolMessage: {tool_call}", to_stdout=False)

        if(tool_call["name"] == "tool_retrive_from_vectordb"):
            context = tool_retrive_from_vectordb(question)
        elif(tool_call["name"] == "tool_retrive_from_keyworddb"):
            context = tool_retrive_from_keyworddb(question)
            #elif(tool_call["name"] == "tool_retrive_from_graphdb"):
            #    #ä»®â†“
            #    context = tool_retrive_from_vectordb(question)
        else:
            raise NotImplementedError(f'Unknown tool call: {tool_call["name"]}')
    #print("context:", context)  # å¤‰æ›ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º

    for i in range(topk):
        #try:
        #page_num = str(int(context[i].metadata["page"]) + 1)
        log_message(logger,f"ã€Source_{i+1}ã€‘: ")
        #log_message(logger,f" - æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«å (ãƒšãƒ¼ã‚¸ç•ªå·) : "+context["context"][i].metadata["source"]+" ("+page_num+")")
        log_message(logger,f" - æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«å : "+context[i].metadata["source"])
        text = context[i].page_content
        text = text.replace("\n", "")
        text = text.replace("\t", "")
        log_message(logger,f" - ãƒ†ã‚­ã‚¹ãƒˆ(æ–‡é ­40å­—): "+text[:40]+" â€¦") 
    #except:
        #    log_message(logger, "Error: ", to_stdout=True)

    step = "å›ç­”"

    #log_message(logger, {str(state)}, to_stdout=False)
    log_message(logger, f"Step: {step}", to_stdout=False)

    return {"context": context,"step": step}

def format_docs(docs:str):
    return "\n\n".join(doc.page_content for doc in docs)

def Node_answer(state: State) -> State:
    """Node for the chatbot to answer questions."""
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥æ‰‹
    context_ = str(state.get("context"))

    question_ = state.get("question")
    #print(f"è³ªå•:{question_}")
    #print(f"æ–‡è„ˆ:{context_}")

    prompt = PromptTemplate(
        variables=["context", "question"],
        template=ANSWER_SYSINT,
    )
    chat_instance_with_rag = prompt | chat_instance | StrOutputParser()

    answer = chat_instance_with_rag.invoke({"context": context_, "question": question_}) 
    #answer = rag_chain_with_source.invoke({"context": context_, "question": question_})
    # 
    #print(answer)  # å¤‰æ›ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º
    step = "ãƒãƒ£ãƒƒãƒˆ"

    #log_message(logger, {str(state)}, to_stdout=False)
    log_message(logger, f"Step: {step} lastMessage: {answer}", to_stdout=False)

    
    return {"answer": answer,"messages": [answer],"step": step}

def Node_human(state: State) -> State:
    last_msg = state["messages"][-1]
    print("ğŸ¤–System:", last_msg.content)
    user_input = input("ğŸ‘¤User: ")
    # If it looks like the user is trying to quit, flag the conversation
    # as over.
    if user_input in {"q"}:
        state["step"] = "çµ‚äº†"
    elif user_input in {"u"}:
        user_input=""
        #state["messages"]=[]
        #stateã‚’å…¨ã¦åˆæœŸåŒ–â†’ä¸å…·åˆã‚ã‚Š
        return {"messages": []}
    else:
        pass

    #log_message(logger, {str(state)}, to_stdout=False)
    log_message(logger, f"Step: {state['step']} lastMessage: {last_msg}", to_stdout=False)

    return state|{"messages": [("user", user_input)]}

# æ¡ä»¶ä»˜ãã‚¨ãƒƒã‚¸
def maybe_route_to_tools(state: State) -> Literal["Node_tool", "Node_human"]:
    """Route between chat and tool nodes if a tool call is made."""
    if not (msgs := state.get("messages", [])):
        raise ValueError(f"No messages found when parsing state: {state}")

    msg = msgs[-1]

    if hasattr(msg, "tool_calls") and len(msg.tool_calls) > 0:
        return "Node_tool"
    else:
        return "Node_human"

def maybe_chat_to_next(state: State) -> Literal["Node_question", "Node_human"]:
    step = state.get("step")
    if(step == "è³ªå•"):
        return "Node_question"
    elif(step == "ãƒãƒ£ãƒƒãƒˆ"):
        return "Node_human"
    else:
        raise ValueError(f"Unknown step: {step}")
    
def maybe_question_to_next(state: State) -> Literal["Node_retriver", "Node_human"]:
    step = state.get("step")
    if(step == "æ¤œç´¢"):
        return "Node_retriver"
    elif(step == "ãƒãƒ£ãƒƒãƒˆ"):
        return "Node_human"
    else:
        raise ValueError(f"Unknown step: {step}")
    
def maybe_exit_human_node(state: State) -> Literal["Node_chatbot", "__end__"]:
    """Route to the chatbot, unless it looks like the user is exiting."""
    if state.get("step") == "çµ‚äº†":
        return END
    else:
        return "Node_chatbot"

# ãƒ„ãƒ¼ãƒ«
@tool
def tool_div_query(query: str) -> str:
    """è³ªå•ã‚’ã‚µãƒ–è³ªå•ã«åˆ†å‰²ã™ã‚‹ãƒ„ãƒ¼ãƒ«"""
    model_instance_query = model_settings.models(chat_model_type) 
    chat_instance_query = model_instance_query.llm
    template=(
        "ä»¥ä¸‹ã®è³ªå•ã‚’ã‚µãƒ–è³ªå•ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚åˆ†å‰²ã—ãŸè³ªå•ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§åŒºåˆ‡ã£ã¦ãã ã•ã„ã€‚\n"
        "è³ªå•: {query}\n"
    )
    prompt = ChatPromptTemplate.from_template(template)

    # LCELã«ã‚ˆã‚‹ãƒã‚§ãƒ¼ãƒ³ä½œæˆ
    rag_chain_from_data = (
        RunnablePassthrough.assign()
        | prompt
        | chat_instance_query
        | StrOutputParser()
    )
    rag_chain_with_source = RunnableParallel(
        {"query": RunnablePassthrough()}
    ).assign(answer=rag_chain_from_data)

    # ãƒãƒ£ãƒƒãƒˆå®Ÿè¡Œ
    result = rag_chain_with_source.invoke(query)
    return result


@tool
def tool_retrive_from_vectordb(question) -> list[Document]:
    """è³ªå•æ–‡ã‚’ã‚‚ã¨ã«ãƒ™ã‚¯ãƒˆãƒ«DBã‹ã‚‰æ–‡æ›¸ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«"""
    # ã“ã“ã«ãƒ™ã‚¯ãƒˆãƒ«DBã‹ã‚‰æ–‡æ›¸ã‚’å–å¾—ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
    inf_param_instance = model_settings.inf_param()
    topk = inf_param_instance.Top_k
    context = set_rag_data_with_vector(question,doc_db_path,emb,topk)
    return context

@tool
def tool_retrive_from_keyworddb(question) -> list[Document]:
    """è³ªå•æ–‡ã‚’ã‚‚ã¨ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰DBã‹ã‚‰æ–‡æ›¸ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«"""
    # ã“ã“ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰DBã‹ã‚‰æ–‡æ›¸ã‚’å–å¾—ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
    inf_param_instance = model_settings.inf_param()
    topk = inf_param_instance.Top_k
    context = set_rag_data_with_keyword(question,doc_db_path,topk)
    return context
if(__name__ == "__main__"):
    chat_model_type = "4o"
    emb_model_type = "azure_emb"
    model_instance = model_settings.models(chat_model_type)  # model_settings.pyãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    # AzureOpenAIã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    chat_instance = model_instance.llm
    chat_instance_question =  model_settings.models(chat_model_type).llm

    chat_instance_retriver =  model_settings.models(chat_model_type).llm
    tools = [tool_retrive_from_vectordb,tool_retrive_from_keyworddb]
    chat_instance_retriver_with_tools = chat_instance_retriver.bind_tools(tools)


    emb_instance = model_settings.embeddings(emb_model_type)  # model_settings.pyãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    emb = emb_instance.emb


    graph_builder = StateGraph(State)
    #ãƒãƒ¼ãƒ‰è¿½åŠ 
    graph_builder.add_node("Node_chatbot", Node_chatbot) # ã‚°ãƒ©ãƒ•ã«Nodeã‚’è¿½åŠ 
    graph_builder.add_node("Node_human", Node_human)
    #graph_builder.add_node("Node_tool", Node_tool)
    graph_builder.add_node("Node_question", Node_question)
    graph_builder.add_node("Node_retriver", Node_retriver)
    graph_builder.add_node("Node_answer", Node_answer)
    #ã‚¨ãƒƒã‚¸è¿½åŠ 
    graph_builder.add_edge(START, "Node_chatbot")
    #graph_builder.add_edge("Node_question","Node_retriver")
    graph_builder.add_edge("Node_retriver","Node_answer")
    graph_builder.add_edge("Node_answer","Node_human")
    
    #æ¡ä»¶ä»˜ãã‚¨ãƒƒã‚¸
    graph_builder.add_conditional_edges("Node_chatbot", maybe_chat_to_next)
    graph_builder.add_conditional_edges("Node_human", maybe_exit_human_node)
    graph_builder.add_conditional_edges("Node_question", maybe_question_to_next)
    
    app = graph_builder.compile()

    graoh_text = "```mermaid\n"+app.get_graph().draw_mermaid() +"```"## mermaidã§CUIã«è¡¨ç¤ºã™ã‚‹å ´åˆ
    with open("../tmp/graph.md", "w") as f:
        f.write(graoh_text)
    config = {"recursion_limit": 100}
    state = app.invoke({"messages": []}, config)